{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "053e3b67",
   "metadata": {},
   "source": [
    "# Model (ResNet) - Binary\n",
    "\n",
    "Within this model, we'll use the same VGG16 model, but reduce the options down to 'Subuaru' and 'Not Subaru'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90019f6",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fffaf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import VGG16,InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import normalize,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee82cf",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b2de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove comment to run again\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=20,\n",
    "# #     width_shift_range=0.15,\n",
    "# #     height_shift_range=0.15,\n",
    "#     shear_range=0.1,\n",
    "#     zoom_range=0.1,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=False,\n",
    "# #     fill_mode='nearest'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c4f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"Datasets/BoundingBoxes/\"\n",
    "\n",
    "# for root, dirs, files in os.walk(image_dir):\n",
    "#     for filename in files:\n",
    "#         if not filename.startswith('.'):\n",
    "#             if os.path.basename(root).startswith('subaru'):\n",
    "#                 filepath = os.path.join(root, filename)\n",
    "#                 # load the image and convert it to a numpy array\n",
    "#                 img = load_img(filepath)\n",
    "#                 img_array = img_to_array(img)\n",
    "#                 # reshape the array to have an extra dimension\n",
    "#                 img_array = img_array.reshape((1,) + img_array.shape)\n",
    "#                 # generate X (in range) augmented images and save them\n",
    "#                 for i in range(4):\n",
    "#                     aug_img_array = datagen.flow(img_array, batch_size=1)[0]\n",
    "#                     aug_img = aug_img_array.astype('uint8')\n",
    "#                     new_filename = f\"{filename.split('.')[0]}_aug{i}.jpg\"\n",
    "#                     new_filepath = os.path.join(root, new_filename)\n",
    "#                     # save the augmented image\n",
    "#                     Image.fromarray(aug_img.squeeze()).save(new_filepath)\n",
    "#                     print(f\"New file created: {new_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70feb6a",
   "metadata": {},
   "source": [
    "### Generator (Data Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0bacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"Datasets/BoundingBoxes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de00ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_generator(image_dir, batch_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for brand_dir in os.listdir(image_dir):\n",
    "        if not brand_dir.startswith('.'):\n",
    "            for filename in os.listdir(os.path.join(image_dir, brand_dir)):\n",
    "                if not filename.startswith('.'):\n",
    "                    label = brand_dir\n",
    "                    filepath = os.path.join(image_dir, brand_dir, filename)\n",
    "                    image = cv2.imread(filepath)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    image = cv2.resize(image, (224, 224))\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "                    if len(images) == batch_size:\n",
    "                        labels = np.array(labels)\n",
    "                        labels = np.where(np.array([label.split('_')[0] for label in labels]) == 'subaru', 1, 0)\n",
    "                        yield np.array(images), np.array(labels)\n",
    "                        images = []\n",
    "                        labels = []\n",
    "    if len(images) > 0:\n",
    "        labels = np.array(labels)\n",
    "        labels = np.where(np.array([label.split('_')[0] for label in labels]) == 'subaru', 1, 0)\n",
    "        yield np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cac46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for batch_images, batch_labels in image_data_generator(image_dir, batch_size=50000):\n",
    "    all_images.append(batch_images)\n",
    "    all_labels.append(batch_labels)\n",
    "\n",
    "all_images = np.concatenate(all_images, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9a33d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207368"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22cddf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115601249951776"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(sum(all_labels)/len(all_labels)) ## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea80571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(all_labels))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b591268",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "To Train-Test split the data, we'll be seperating X on index to prevent all of the data from being loaded into the kernel concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31654b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(all_images))\n",
    "train_idx, test_idx = train_test_split(idx, test_size=0.2, stratify=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ec6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = all_images[train_idx], all_labels[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a442029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = all_images[test_idx], all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411cd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.reshape(y_train, (-1, 1))\n",
    "# y_test = np.reshape(y_test, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8646a03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41474"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b238fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subaru count: 64440\n",
      "Other count: 101454\n"
     ]
    }
   ],
   "source": [
    "print('Subaru count:', np.sum(y_train))\n",
    "print('Other count:', np.sum(1 - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8dd255",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "\n",
    "We'll feed our data into the model in batches of 32 so that all images aren’t loaded into the kernel at the same time. Within this function, we'll divide X_low by 255, to normalize the data.\n",
    "\n",
    "This operation is split into two seperate generators to prevent data leakage from the training set into the validation set. Since the function has been seperated, each function will only be called when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004c1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(X_train, y_train, batch_size):\n",
    "    num_train_samples = len(X_train)\n",
    "    train_indices = np.arange(num_train_samples)\n",
    "    np.random.shuffle(train_indices)\n",
    "\n",
    "    while True:\n",
    "        for start_idx in range(0, num_train_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_train_samples)\n",
    "            batch_indices = train_indices[start_idx:end_idx]\n",
    "            X_batch_train = torch.tensor(X_train[batch_indices], dtype=torch.float32)\n",
    "            y_batch_train = torch.tensor(y_train[batch_indices], dtype=torch.long)\n",
    "\n",
    "#             # Normalize the input data to [0, 1]\n",
    "#             X_batch_train /= 255.0\n",
    "\n",
    "            yield X_batch_train, y_batch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a72f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_generator(X_test, y_test, batch_size):\n",
    "    num_test_samples = len(X_test)\n",
    "    test_indices = torch.randperm(num_test_samples)\n",
    "    while True:\n",
    "        for start_idx_test in range(0, num_test_samples, batch_size):\n",
    "            end_idx_test = min(start_idx_test + batch_size, num_test_samples)\n",
    "            batch_indices_test = test_indices[start_idx_test:end_idx_test]\n",
    "            batch_X_test = torch.tensor(X_test[batch_indices_test], dtype=torch.float32)\n",
    "            batch_y_test = torch.tensor(y_test[batch_indices_test], dtype=torch.long)\n",
    "            yield batch_X_test, batch_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a989c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_generator = train_data_generator(X_train, y_train, batch_size)\n",
    "val_generator = test_data_generator(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be219691",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(X_train) // batch_size\n",
    "val_steps = len(X_test) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e60cc",
   "metadata": {},
   "source": [
    "### Instantiating VGG16 Model (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37997859",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics=['accuracy']\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e02f946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 19s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "584ee176",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a77df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= optimizer,\n",
    "              loss= loss_fn,\n",
    "              metrics= metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c439517",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53f06e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 16:39:08.956387: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5184/5184 [==============================] - ETA: 0s - loss: 1.8429 - accuracy: 0.8279"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 16:44:51.557365: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5184/5184 [==============================] - 426s 81ms/step - loss: 1.8429 - accuracy: 0.8279 - val_loss: 1.6169 - val_accuracy: 0.8636\n",
      "Epoch 2/50\n",
      "5184/5184 [==============================] - 416s 80ms/step - loss: 1.4853 - accuracy: 0.8579 - val_loss: 1.2034 - val_accuracy: 0.8820\n",
      "Epoch 3/50\n",
      "5184/5184 [==============================] - 415s 80ms/step - loss: 1.4040 - accuracy: 0.8672 - val_loss: 1.7208 - val_accuracy: 0.8379\n",
      "Epoch 4/50\n",
      "5184/5184 [==============================] - 418s 81ms/step - loss: 1.3674 - accuracy: 0.8738 - val_loss: 1.7193 - val_accuracy: 0.8351\n",
      "Epoch 5/50\n",
      "5184/5184 [==============================] - 418s 81ms/step - loss: 1.3531 - accuracy: 0.8768 - val_loss: 1.5548 - val_accuracy: 0.8529\n",
      "Epoch 6/50\n",
      "5184/5184 [==============================] - 421s 81ms/step - loss: 1.3480 - accuracy: 0.8797 - val_loss: 1.2469 - val_accuracy: 0.8823\n",
      "Epoch 7/50\n",
      "5184/5184 [==============================] - 416s 80ms/step - loss: 1.3052 - accuracy: 0.8830 - val_loss: 1.3140 - val_accuracy: 0.8802\n",
      "Epoch 8/50\n",
      "5184/5184 [==============================] - 415s 80ms/step - loss: 1.2358 - accuracy: 0.8865 - val_loss: 1.3070 - val_accuracy: 0.8831\n",
      "Epoch 9/50\n",
      "5184/5184 [==============================] - 416s 80ms/step - loss: 1.3445 - accuracy: 0.8844 - val_loss: 1.8331 - val_accuracy: 0.8834\n",
      "Epoch 10/50\n",
      "5184/5184 [==============================] - 416s 80ms/step - loss: 1.2751 - accuracy: 0.8880 - val_loss: 2.0218 - val_accuracy: 0.8737\n",
      "Epoch 11/50\n",
      "5184/5184 [==============================] - 417s 80ms/step - loss: 1.2641 - accuracy: 0.8891 - val_loss: 1.5605 - val_accuracy: 0.8946\n",
      "Epoch 12/50\n",
      "5184/5184 [==============================] - 419s 81ms/step - loss: 1.2882 - accuracy: 0.8902 - val_loss: 1.9551 - val_accuracy: 0.8903\n",
      "Epoch 13/50\n",
      "5184/5184 [==============================] - 416s 80ms/step - loss: 1.3136 - accuracy: 0.8911 - val_loss: 1.5729 - val_accuracy: 0.8972\n",
      "Epoch 14/50\n",
      "5184/5184 [==============================] - 414s 80ms/step - loss: 1.2902 - accuracy: 0.8926 - val_loss: 2.0963 - val_accuracy: 0.8484\n",
      "Epoch 15/50\n",
      "5184/5184 [==============================] - 415s 80ms/step - loss: 1.2834 - accuracy: 0.8934 - val_loss: 2.0702 - val_accuracy: 0.8648\n",
      "Epoch 16/50\n",
      "5184/5184 [==============================] - 416s 80ms/step - loss: 1.3524 - accuracy: 0.8935 - val_loss: 1.9522 - val_accuracy: 0.8738\n",
      "Epoch 17/50\n",
      "5184/5184 [==============================] - 419s 81ms/step - loss: 1.3364 - accuracy: 0.8953 - val_loss: 1.6672 - val_accuracy: 0.8959\n",
      "Epoch 18/50\n",
      "5184/5184 [==============================] - 400s 77ms/step - loss: 1.1181 - accuracy: 0.9024 - val_loss: 1.5160 - val_accuracy: 0.8924\n",
      "Epoch 19/50\n",
      "5184/5184 [==============================] - 353s 68ms/step - loss: 1.1242 - accuracy: 0.9017 - val_loss: 1.5138 - val_accuracy: 0.8950\n",
      "Epoch 20/50\n",
      "5184/5184 [==============================] - 354s 68ms/step - loss: 1.1418 - accuracy: 0.8999 - val_loss: 2.1475 - val_accuracy: 0.8890\n",
      "Epoch 21/50\n",
      "5184/5184 [==============================] - 349s 67ms/step - loss: 1.2042 - accuracy: 0.9001 - val_loss: 1.9017 - val_accuracy: 0.8937\n",
      "Epoch 23/50\n",
      "5184/5184 [==============================] - 350s 67ms/step - loss: 1.2839 - accuracy: 0.8982 - val_loss: 2.1267 - val_accuracy: 0.8731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x41a3d49d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          epochs=50,\n",
    "          validation_data=val_generator,\n",
    "          callbacks=[early_stopping],\n",
    "          steps_per_epoch=train_steps, \n",
    "          validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ab09b",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef5da4",
   "metadata": {},
   "source": [
    "* To do:\n",
    " - seperate train/test datasets\n",
    " - create pytorch dataloaders for each datasets like\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put them in a dictionary\n",
    "dataloaders = {'train': train_data_generator, 'valid': test_data_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 53 * 53, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=5, device='cuda'):\n",
    "    start = time.time()\n",
    "    train_results = []\n",
    "    valid_results = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':            \n",
    "              model.train()  # Set model to training mode\n",
    "            else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if(phase == 'train'):\n",
    "              train_results.append([epoch_loss,epoch_acc])\n",
    "            if(phase == 'valid'):\n",
    "              valid_results.append([epoch_loss,epoch_acc])\n",
    "                                   \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model (Early Stopping) and Saving our model, when we get best accuracy\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())       \n",
    "                model_save_name = \"resnetCars.pt\"\n",
    "                path = F\"/content/drive/My Drive/{model_save_name}\"\n",
    "                torch.save(model.state_dict(), path)        \n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculating time it took for model to train    \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, train_results, valid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b465e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model, train_results, valid_results = train_model(model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09a2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b2352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3408116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1022e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5699deba",
   "metadata": {},
   "source": [
    "### Gradient Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.hstack((y_train, 1 - y_train))\n",
    "y_test = np.hstack((y_test, 1 - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_data_generator, X_train, y_train, batch_size, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        epoch_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        batches = train_data_generator(X_train, y_train, batch_size)\n",
    "        with tqdm(total=len(X_train)//batch_size) as pbar:\n",
    "            for i, (X_batch_train, y_batch_train) in enumerate(batches):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch_train, training=True)\n",
    "                    batch_loss = loss_fn(y_batch_train, y_pred)\n",
    "                gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                batch_accuracy = accuracy_score(np.argmax(y_batch_train, axis=1), np.argmax(y_pred.numpy(), axis=1))\n",
    "                epoch_loss.assign_add(tf.reduce_sum(batch_loss))\n",
    "                epoch_accuracy.assign_add(batch_accuracy)\n",
    "                pbar.update(1)\n",
    "        epoch_loss = epoch_loss / (X_train.shape[0] // batch_size)\n",
    "        epoch_accuracy = epoch_accuracy / (X_train.shape[0] // batch_size)\n",
    "        print(f\"Epoch {epoch + 1}: loss={epoch_loss}, accuracy={epoch_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_data_generator, X_test, y_test, batch_size):\n",
    "\n",
    "    # Initialize the loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "\n",
    "    # Iterate over the batches in the test data generator\n",
    "    for X_batch_test, y_batch_test in test_data_generator(X_test, y_test, batch_size):\n",
    "\n",
    "        # Compute the predictions and loss for the batch\n",
    "        y_pred = model(X_batch_test, training=False)\n",
    "        batch_loss = loss_fn(y_batch_test, y_pred)\n",
    "\n",
    "        # Compute the batch accuracy\n",
    "        batch_accuracy = accuracy_score(np.argmax(y_batch_train.numpy(), axis=1), np.argmax(y_pred.numpy(), axis=1))\n",
    "\n",
    "        # Update the test loss and accuracy\n",
    "        test_loss += batch_loss.numpy()\n",
    "        test_accuracy += batch_accuracy\n",
    "\n",
    "    # Compute the average test loss and accuracy\n",
    "    test_loss /= (len(X_test) / batch_size)\n",
    "    test_accuracy /= (len(X_test) / batch_size)\n",
    "\n",
    "    # Print the test loss and accuracy\n",
    "    print(\"Test loss: {:.4f} - Test accuracy: {:.4f}\".format(test_loss, test_accuracy))\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2621b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, binary_crossentropy, train_data_generator, X_train, y_train, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0356b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data_generator(X_train, y_train, batch_size),\n",
    "    validation_data=test_data_generator(X_test, y_test, batch_size),\n",
    "    epochs=epochs, \n",
    "    steps_per_epoch=train_steps, \n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(model, loss_fn, test_data_generator, X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe680a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297/1297 [==============================] - 64s 49ms/step\n",
      "1297/1297 [==============================] - 67s 50ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.round(y_pred)\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "# Classify examples based on the threshold\n",
    "y_pred = np.where(y_pred_prob > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0880a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4571002892115334\n",
      "Recall: 0.971260086902545\n",
      "Accuracy: 0.5407484206973043\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5455aa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41ffaaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6759, 18605],\n",
       "       [  455, 15655]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "834351d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e5a759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5404349713073251,\n",
       " 0.45694687682428486,\n",
       " 0.9717566728739913,\n",
       " 0.6216001588246973)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ace89",
   "metadata": {},
   "source": [
    "1) ResNet150 Model\n",
    "    - bounding boxes with multiple augmentations completed for subaru images\n",
    "    - acc,prec,recall,f1_score (0.5754930800019289,0.47765162832387215,0.9923649906890131,0.6448971359419121)\n",
    "    - baseline .625 (base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    - converges to .999999 accuracy on model, .989 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe719ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

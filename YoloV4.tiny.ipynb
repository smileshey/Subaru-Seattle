{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc070233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import normalize,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31055f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"Datasets/VMMRdb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa67f59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-05 14:34:08--  https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 625 [text/plain]\n",
      "Saving to: ‘coco.names’\n",
      "\n",
      "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
      "\n",
      "2023-05-05 14:34:08 (6.85 MB/s) - ‘coco.names’ saved [625/625]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put them in a dictionary\n",
    "dataloaders = {'train': train_data_generator, 'valid': test_data_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b430d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 53 * 53, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0052b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=5, device='cuda'):\n",
    "    start = time.time()\n",
    "    train_results = []\n",
    "    valid_results = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':            \n",
    "              model.train()  # Set model to training mode\n",
    "            else:\n",
    "              model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if(phase == 'train'):\n",
    "              train_results.append([epoch_loss,epoch_acc])\n",
    "            if(phase == 'valid'):\n",
    "              valid_results.append([epoch_loss,epoch_acc])\n",
    "                                   \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model (Early Stopping) and Saving our model, when we get best accuracy\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())       \n",
    "                model_save_name = \"resnetCars.pt\"\n",
    "                path = F\"/content/drive/My Drive/{model_save_name}\"\n",
    "                torch.save(model.state_dict(), path)        \n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculating time it took for model to train    \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, train_results, valid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208004cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model, train_results, valid_results = train_model(model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.hstack((y_train, 1 - y_train))\n",
    "y_test = np.hstack((y_test, 1 - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5686db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_data_generator, X_train, y_train, batch_size, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        epoch_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        batches = train_data_generator(X_train, y_train, batch_size)\n",
    "        with tqdm(total=len(X_train)//batch_size) as pbar:\n",
    "            for i, (X_batch_train, y_batch_train) in enumerate(batches):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch_train, training=True)\n",
    "                    batch_loss = loss_fn(y_batch_train, y_pred)\n",
    "                gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                batch_accuracy = accuracy_score(np.argmax(y_batch_train, axis=1), np.argmax(y_pred.numpy(), axis=1))\n",
    "                epoch_loss.assign_add(tf.reduce_sum(batch_loss))\n",
    "                epoch_accuracy.assign_add(batch_accuracy)\n",
    "                pbar.update(1)\n",
    "        epoch_loss = epoch_loss / (X_train.shape[0] // batch_size)\n",
    "        epoch_accuracy = epoch_accuracy / (X_train.shape[0] // batch_size)\n",
    "        print(f\"Epoch {epoch + 1}: loss={epoch_loss}, accuracy={epoch_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_data_generator, X_test, y_test, batch_size):\n",
    "\n",
    "    # Initialize the loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "\n",
    "    # Iterate over the batches in the test data generator\n",
    "    for X_batch_test, y_batch_test in test_data_generator(X_test, y_test, batch_size):\n",
    "\n",
    "        # Compute the predictions and loss for the batch\n",
    "        y_pred = model(X_batch_test, training=False)\n",
    "        batch_loss = loss_fn(y_batch_test, y_pred)\n",
    "\n",
    "        # Compute the batch accuracy\n",
    "        batch_accuracy = accuracy_score(np.argmax(y_batch_train.numpy(), axis=1), np.argmax(y_pred.numpy(), axis=1))\n",
    "\n",
    "        # Update the test loss and accuracy\n",
    "        test_loss += batch_loss.numpy()\n",
    "        test_accuracy += batch_accuracy\n",
    "\n",
    "    # Compute the average test loss and accuracy\n",
    "    test_loss /= (len(X_test) / batch_size)\n",
    "    test_accuracy /= (len(X_test) / batch_size)\n",
    "\n",
    "    # Print the test loss and accuracy\n",
    "    print(\"Test loss: {:.4f} - Test accuracy: {:.4f}\".format(test_loss, test_accuracy))\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, binary_crossentropy, train_data_generator, X_train, y_train, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e868610",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data_generator(X_train, y_train, batch_size),\n",
    "    validation_data=test_data_generator(X_test, y_test, batch_size),\n",
    "    epochs=epochs, \n",
    "    steps_per_epoch=train_steps, \n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a717083",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(model, loss_fn, test_data_generator, X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d1441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9888e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca179df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

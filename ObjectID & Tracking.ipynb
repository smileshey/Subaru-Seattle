{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93643d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "import colorsys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorflow.keras.applications import VGG16,InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import normalize,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.tools import generate_detections as gdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bad7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68d3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d51c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad37e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c47a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eba098",
   "metadata": {},
   "source": [
    "### Loading Trained InceptionV3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a0adcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 08:29:25.658170: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-29 08:29:25.658590: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = tf.saved_model.load('models/savedModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83f64f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_filename = \"models/graphs/InceptionCustom/output_graph.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa1eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 08:29:29.798216: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-29 08:29:29.798236: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model_path = graph_filename\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(model_path, 'rb') as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "# Start a TensorFlow session\n",
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    # Get the output node names\n",
    "    output_node_names = [node.name for node in graph_def.node if node.op == 'Placeholder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4ff1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 08:29:29.972875: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-29 08:29:29.980748: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-24412e14fe8584d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-24412e14fe8584d5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_model = model\n",
    "\n",
    "with tf.summary.create_file_writer('logs/').as_default():\n",
    "    # Enable tracing of TensorFlow operations\n",
    "    tf.summary.trace_on(graph=True)\n",
    "\n",
    "    # Execute a forward pass on the model to capture the graph\n",
    "    inputs = tf.ones([1, 224, 224, 3])  # Provide a sample input\n",
    "    _ = loaded_model(inputs)  # Forward pass\n",
    "\n",
    "    # Export the captured graph to the log directory\n",
    "    tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir='logs/')\n",
    "\n",
    "# Launch TensorBoard with the specified log directory\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff13633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Signatures: \n",
      "inception_v3_input:0\n",
      "unknown:0\n",
      "unknown_0:0\n",
      "unknown_1:0\n",
      "unknown_2:0\n",
      "unknown_3:0\n",
      "unknown_4:0\n",
      "unknown_5:0\n",
      "unknown_6:0\n",
      "unknown_7:0\n",
      "unknown_8:0\n",
      "unknown_9:0\n",
      "unknown_10:0\n",
      "unknown_11:0\n",
      "unknown_12:0\n",
      "unknown_13:0\n",
      "unknown_14:0\n",
      "unknown_15:0\n",
      "unknown_16:0\n",
      "unknown_17:0\n",
      "unknown_18:0\n",
      "unknown_19:0\n",
      "unknown_20:0\n",
      "unknown_21:0\n",
      "unknown_22:0\n",
      "unknown_23:0\n",
      "unknown_24:0\n",
      "unknown_25:0\n",
      "unknown_26:0\n",
      "unknown_27:0\n",
      "unknown_28:0\n",
      "unknown_29:0\n",
      "unknown_30:0\n",
      "unknown_31:0\n",
      "unknown_32:0\n",
      "unknown_33:0\n",
      "unknown_34:0\n",
      "unknown_35:0\n",
      "unknown_36:0\n",
      "unknown_37:0\n",
      "unknown_38:0\n",
      "unknown_39:0\n",
      "unknown_40:0\n",
      "unknown_41:0\n",
      "unknown_42:0\n",
      "unknown_43:0\n",
      "unknown_44:0\n",
      "unknown_45:0\n",
      "unknown_46:0\n",
      "unknown_47:0\n",
      "unknown_48:0\n",
      "unknown_49:0\n",
      "unknown_50:0\n",
      "unknown_51:0\n",
      "unknown_52:0\n",
      "unknown_53:0\n",
      "unknown_54:0\n",
      "unknown_55:0\n",
      "unknown_56:0\n",
      "unknown_57:0\n",
      "unknown_58:0\n",
      "unknown_59:0\n",
      "unknown_60:0\n",
      "unknown_61:0\n",
      "unknown_62:0\n",
      "unknown_63:0\n",
      "unknown_64:0\n",
      "unknown_65:0\n",
      "unknown_66:0\n",
      "unknown_67:0\n",
      "unknown_68:0\n",
      "unknown_69:0\n",
      "unknown_70:0\n",
      "unknown_71:0\n",
      "unknown_72:0\n",
      "unknown_73:0\n",
      "unknown_74:0\n",
      "unknown_75:0\n",
      "unknown_76:0\n",
      "unknown_77:0\n",
      "unknown_78:0\n",
      "unknown_79:0\n",
      "unknown_80:0\n",
      "unknown_81:0\n",
      "unknown_82:0\n",
      "unknown_83:0\n",
      "unknown_84:0\n",
      "unknown_85:0\n",
      "unknown_86:0\n",
      "unknown_87:0\n",
      "unknown_88:0\n",
      "unknown_89:0\n",
      "unknown_90:0\n",
      "unknown_91:0\n",
      "unknown_92:0\n",
      "unknown_93:0\n",
      "unknown_94:0\n",
      "unknown_95:0\n",
      "unknown_96:0\n",
      "unknown_97:0\n",
      "unknown_98:0\n",
      "unknown_99:0\n",
      "unknown_100:0\n",
      "unknown_101:0\n",
      "unknown_102:0\n",
      "unknown_103:0\n",
      "unknown_104:0\n",
      "unknown_105:0\n",
      "unknown_106:0\n",
      "unknown_107:0\n",
      "unknown_108:0\n",
      "unknown_109:0\n",
      "unknown_110:0\n",
      "unknown_111:0\n",
      "unknown_112:0\n",
      "unknown_113:0\n",
      "unknown_114:0\n",
      "unknown_115:0\n",
      "unknown_116:0\n",
      "unknown_117:0\n",
      "unknown_118:0\n",
      "unknown_119:0\n",
      "unknown_120:0\n",
      "unknown_121:0\n",
      "unknown_122:0\n",
      "unknown_123:0\n",
      "unknown_124:0\n",
      "unknown_125:0\n",
      "unknown_126:0\n",
      "unknown_127:0\n",
      "unknown_128:0\n",
      "unknown_129:0\n",
      "unknown_130:0\n",
      "unknown_131:0\n",
      "unknown_132:0\n",
      "unknown_133:0\n",
      "unknown_134:0\n",
      "unknown_135:0\n",
      "unknown_136:0\n",
      "unknown_137:0\n",
      "unknown_138:0\n",
      "unknown_139:0\n",
      "unknown_140:0\n",
      "unknown_141:0\n",
      "unknown_142:0\n",
      "unknown_143:0\n",
      "unknown_144:0\n",
      "unknown_145:0\n",
      "unknown_146:0\n",
      "unknown_147:0\n",
      "unknown_148:0\n",
      "unknown_149:0\n",
      "unknown_150:0\n",
      "unknown_151:0\n",
      "unknown_152:0\n",
      "unknown_153:0\n",
      "unknown_154:0\n",
      "unknown_155:0\n",
      "unknown_156:0\n",
      "unknown_157:0\n",
      "unknown_158:0\n",
      "unknown_159:0\n",
      "unknown_160:0\n",
      "unknown_161:0\n",
      "unknown_162:0\n",
      "unknown_163:0\n",
      "unknown_164:0\n",
      "unknown_165:0\n",
      "unknown_166:0\n",
      "unknown_167:0\n",
      "unknown_168:0\n",
      "unknown_169:0\n",
      "unknown_170:0\n",
      "unknown_171:0\n",
      "unknown_172:0\n",
      "unknown_173:0\n",
      "unknown_174:0\n",
      "unknown_175:0\n",
      "unknown_176:0\n",
      "unknown_177:0\n",
      "unknown_178:0\n",
      "unknown_179:0\n",
      "unknown_180:0\n",
      "unknown_181:0\n",
      "unknown_182:0\n",
      "unknown_183:0\n",
      "unknown_184:0\n",
      "unknown_185:0\n",
      "unknown_186:0\n",
      "unknown_187:0\n",
      "unknown_188:0\n",
      "unknown_189:0\n",
      "unknown_190:0\n",
      "unknown_191:0\n",
      "unknown_192:0\n",
      "unknown_193:0\n",
      "unknown_194:0\n",
      "unknown_195:0\n",
      "unknown_196:0\n",
      "unknown_197:0\n",
      "unknown_198:0\n",
      "unknown_199:0\n",
      "unknown_200:0\n",
      "unknown_201:0\n",
      "unknown_202:0\n",
      "unknown_203:0\n",
      "unknown_204:0\n",
      "unknown_205:0\n",
      "unknown_206:0\n",
      "unknown_207:0\n",
      "unknown_208:0\n",
      "unknown_209:0\n",
      "unknown_210:0\n",
      "unknown_211:0\n",
      "unknown_212:0\n",
      "unknown_213:0\n",
      "unknown_214:0\n",
      "unknown_215:0\n",
      "unknown_216:0\n",
      "unknown_217:0\n",
      "unknown_218:0\n",
      "unknown_219:0\n",
      "unknown_220:0\n",
      "unknown_221:0\n",
      "unknown_222:0\n",
      "unknown_223:0\n",
      "unknown_224:0\n",
      "unknown_225:0\n",
      "unknown_226:0\n",
      "unknown_227:0\n",
      "unknown_228:0\n",
      "unknown_229:0\n",
      "unknown_230:0\n",
      "unknown_231:0\n",
      "unknown_232:0\n",
      "unknown_233:0\n",
      "unknown_234:0\n",
      "unknown_235:0\n",
      "unknown_236:0\n",
      "unknown_237:0\n",
      "unknown_238:0\n",
      "unknown_239:0\n",
      "unknown_240:0\n",
      "unknown_241:0\n",
      "unknown_242:0\n",
      "unknown_243:0\n",
      "unknown_244:0\n",
      "unknown_245:0\n",
      "unknown_246:0\n",
      "unknown_247:0\n",
      "unknown_248:0\n",
      "unknown_249:0\n",
      "unknown_250:0\n",
      "unknown_251:0\n",
      "unknown_252:0\n",
      "unknown_253:0\n",
      "unknown_254:0\n",
      "unknown_255:0\n",
      "unknown_256:0\n",
      "unknown_257:0\n",
      "unknown_258:0\n",
      "unknown_259:0\n",
      "unknown_260:0\n",
      "unknown_261:0\n",
      "unknown_262:0\n",
      "unknown_263:0\n",
      "unknown_264:0\n",
      "unknown_265:0\n",
      "unknown_266:0\n",
      "unknown_267:0\n",
      "unknown_268:0\n",
      "unknown_269:0\n",
      "unknown_270:0\n",
      "unknown_271:0\n",
      "unknown_272:0\n",
      "unknown_273:0\n",
      "unknown_274:0\n",
      "unknown_275:0\n",
      "unknown_276:0\n",
      "unknown_277:0\n",
      "unknown_278:0\n",
      "unknown_279:0\n",
      "unknown_280:0\n",
      "unknown_281:0\n",
      "unknown_282:0\n",
      "unknown_283:0\n",
      "unknown_284:0\n",
      "unknown_285:0\n",
      "unknown_286:0\n",
      "unknown_287:0\n",
      "unknown_288:0\n",
      "unknown_289:0\n",
      "unknown_290:0\n",
      "unknown_291:0\n",
      "unknown_292:0\n",
      "unknown_293:0\n",
      "unknown_294:0\n",
      "unknown_295:0\n",
      "unknown_296:0\n",
      "unknown_297:0\n",
      "unknown_298:0\n",
      "unknown_299:0\n",
      "unknown_300:0\n",
      "unknown_301:0\n",
      "unknown_302:0\n",
      "unknown_303:0\n",
      "unknown_304:0\n",
      "unknown_305:0\n",
      "unknown_306:0\n",
      "unknown_307:0\n",
      "unknown_308:0\n",
      "unknown_309:0\n",
      "unknown_310:0\n",
      "unknown_311:0\n",
      "unknown_312:0\n",
      "unknown_313:0\n",
      "unknown_314:0\n",
      "unknown_315:0\n",
      "unknown_316:0\n",
      "unknown_317:0\n",
      "unknown_318:0\n",
      "unknown_319:0\n",
      "unknown_320:0\n",
      "unknown_321:0\n",
      "unknown_322:0\n",
      "unknown_323:0\n",
      "unknown_324:0\n",
      "unknown_325:0\n",
      "unknown_326:0\n",
      "unknown_327:0\n",
      "unknown_328:0\n",
      "unknown_329:0\n",
      "unknown_330:0\n",
      "unknown_331:0\n",
      "unknown_332:0\n",
      "unknown_333:0\n",
      "unknown_334:0\n",
      "unknown_335:0\n",
      "unknown_336:0\n",
      "unknown_337:0\n",
      "unknown_338:0\n",
      "unknown_339:0\n",
      "unknown_340:0\n",
      "unknown_341:0\n",
      "unknown_342:0\n",
      "unknown_343:0\n",
      "unknown_344:0\n",
      "unknown_345:0\n",
      "unknown_346:0\n",
      "unknown_347:0\n",
      "unknown_348:0\n",
      "unknown_349:0\n",
      "unknown_350:0\n",
      "unknown_351:0\n",
      "unknown_352:0\n",
      "unknown_353:0\n",
      "unknown_354:0\n",
      "unknown_355:0\n",
      "unknown_356:0\n",
      "unknown_357:0\n",
      "unknown_358:0\n",
      "unknown_359:0\n",
      "unknown_360:0\n",
      "unknown_361:0\n",
      "unknown_362:0\n",
      "unknown_363:0\n",
      "unknown_364:0\n",
      "unknown_365:0\n",
      "unknown_366:0\n",
      "unknown_367:0\n",
      "unknown_368:0\n",
      "unknown_369:0\n",
      "unknown_370:0\n",
      "unknown_371:0\n",
      "unknown_372:0\n",
      "unknown_373:0\n",
      "unknown_374:0\n",
      "unknown_375:0\n",
      "unknown_376:0\n",
      "unknown_377:0\n",
      "unknown_378:0\n",
      "unknown_379:0\n",
      "unknown_380:0\n",
      "unknown_381:0\n",
      "unknown_382:0\n",
      "unknown_383:0\n",
      "unknown_384:0\n",
      "unknown_385:0\n",
      "unknown_386:0\n",
      "unknown_387:0\n",
      "unknown_388:0\n",
      "unknown_389:0\n",
      "unknown_390:0\n",
      "unknown_391:0\n",
      "unknown_392:0\n",
      "unknown_393:0\n",
      "unknown_394:0\n",
      "unknown_395:0\n",
      "unknown_396:0\n",
      "unknown_397:0\n",
      "unknown_398:0\n",
      "unknown_399:0\n",
      "unknown_400:0\n",
      "unknown_401:0\n",
      "unknown_402:0\n",
      "unknown_403:0\n",
      "unknown_404:0\n",
      "unknown_405:0\n",
      "unknown_406:0\n",
      "unknown_407:0\n",
      "unknown_408:0\n",
      "unknown_409:0\n",
      "unknown_410:0\n",
      "unknown_411:0\n",
      "unknown_412:0\n",
      "unknown_413:0\n",
      "unknown_414:0\n",
      "unknown_415:0\n",
      "unknown_416:0\n",
      "unknown_417:0\n",
      "unknown_418:0\n",
      "unknown_419:0\n",
      "unknown_420:0\n",
      "unknown_421:0\n",
      "unknown_422:0\n",
      "unknown_423:0\n",
      "unknown_424:0\n",
      "unknown_425:0\n",
      "unknown_426:0\n",
      "unknown_427:0\n",
      "unknown_428:0\n",
      "unknown_429:0\n",
      "unknown_430:0\n",
      "unknown_431:0\n",
      "unknown_432:0\n",
      "unknown_433:0\n",
      "unknown_434:0\n",
      "unknown_435:0\n",
      "unknown_436:0\n",
      "unknown_437:0\n",
      "unknown_438:0\n",
      "unknown_439:0\n",
      "unknown_440:0\n",
      "unknown_441:0\n",
      "unknown_442:0\n",
      "unknown_443:0\n",
      "unknown_444:0\n",
      "unknown_445:0\n",
      "unknown_446:0\n",
      "unknown_447:0\n",
      "unknown_448:0\n",
      "unknown_449:0\n",
      "unknown_450:0\n",
      "unknown_451:0\n",
      "unknown_452:0\n",
      "unknown_453:0\n",
      "unknown_454:0\n",
      "unknown_455:0\n",
      "unknown_456:0\n",
      "unknown_457:0\n",
      "unknown_458:0\n",
      "unknown_459:0\n",
      "unknown_460:0\n",
      "unknown_461:0\n",
      "unknown_462:0\n",
      "unknown_463:0\n",
      "unknown_464:0\n",
      "unknown_465:0\n",
      "unknown_466:0\n",
      "unknown_467:0\n",
      "unknown_468:0\n",
      "unknown_469:0\n",
      "unknown_470:0\n",
      "\n",
      "Output Signatures: \n",
      "Identity:0\n"
     ]
    }
   ],
   "source": [
    "def print_io(saved_model_dir):\n",
    "\n",
    "    # Load the model\n",
    "    loaded_model = tf.saved_model.load(saved_model_dir)\n",
    "\n",
    "    # Get the input signature of the model\n",
    "    input_signature = loaded_model.signatures['serving_default'].inputs\n",
    "    print(\"\\nInput Signatures: \")\n",
    "    for input in input_signature:\n",
    "        print(input.name)\n",
    "\n",
    "    # Get the output signature of the model\n",
    "    output_signature = loaded_model.signatures['serving_default'].outputs\n",
    "    print(\"\\nOutput Signatures: \")\n",
    "    for output in output_signature:\n",
    "        print(output.name)\n",
    "                \n",
    "print_io(\"models/savedModels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee1cded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, inception_v3_input) at 0x2DC93E8B0>}))\n"
     ]
    }
   ],
   "source": [
    "print(model.signatures.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ffac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dense'])\n"
     ]
    }
   ],
   "source": [
    "signature = model.signatures['serving_default']\n",
    "print(signature.structured_outputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24d6c7",
   "metadata": {},
   "source": [
    "### Loading Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0134ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_model = cv2.dnn.readNet(\"models/yolov4-tiny.weights\", \"models/yolov4-tiny.cfg\")\n",
    "object_detection_model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "object_detection_model.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea4389",
   "metadata": {},
   "source": [
    "### Reading in Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69eb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'Datasets/Video/QueenAnnRoy/moving/test.mov'\n",
    "output_dir = 'Datasets/Video/QueenAnnRoy/stills_test/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9153e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Subaru', 'car']\n",
    "# with open(\"coco.names\", \"r\") as f:\n",
    "#     classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f4ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "input_size = (244, 244)\n",
    "input_scale = 1/255.0\n",
    "confidence_thresh = .1\n",
    "overlap = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809a9a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subaru', 'car']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d4d6ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<VideoCapture 0x2bbba0b70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8793c",
   "metadata": {},
   "source": [
    "### Defining Region of Interest (ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50359ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_coordinates = [(352, 820), (855, 820), (1282, 270),(1082, 270)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d394c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary mask image with the same size as the original image\n",
    "image = cv2.imread('Datasets/Video/QueenAnnRoy/stills/frame_000000.jpg')  # Load your image\n",
    "mask = np.zeros_like(image)\n",
    "\n",
    "# Convert the coords to numpy\n",
    "roi_pts = np.array(roi_coordinates, dtype=np.int32)\n",
    "cv2.fillPoly(mask, [roi_pts], (255, 255, 255))  # Set ROI pixels to white (255)\n",
    "\n",
    "roi = cv2.bitwise_and(image, mask)\n",
    "\n",
    "# cv2.imshow('ROI', roi)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9fa0b",
   "metadata": {},
   "source": [
    "### Initializing DeepSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01877a7e",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a0a059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, input_size)\n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66450b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold, CLASSES):\n",
    "    height, width = original_image.shape[:2]\n",
    "    filtered_boxes = []\n",
    "    for pred in pred_bbox:\n",
    "        if np.any(np.greater(pred[4], score_threshold)):\n",
    "            xmin = int(pred[0][0] * width)\n",
    "            ymin = int(pred[0][1] * height)\n",
    "            xmax = int(pred[0][2] * width)\n",
    "            ymax = int(pred[0][3] * height)\n",
    "            class_index = int(pred[0][5])\n",
    "            class_name = CLASSES[class_index]\n",
    "            score = float(pred[0][4])\n",
    "            filtered_boxes.append([xmin, ymin, xmax, ymax, class_name, score])\n",
    "    return filtered_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827c5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, overlap, method='nms'):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Extract coordinates and scores\n",
    "    x1 = boxes[:, 0].astype(float)\n",
    "    y1 = boxes[:, 1].astype(float)\n",
    "    x2 = boxes[:, 2].astype(float)\n",
    "    y2 = boxes[:, 3].astype(float)\n",
    "    scores = boxes[:, 5]\n",
    "\n",
    "    # Calculate areas of bounding boxes\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    # Sort boxes by their scores in descending order\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    # Initialize list for selected bounding boxes\n",
    "    selected_boxes = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        # Select box with the highest score\n",
    "        max_idx = order[0]\n",
    "        selected_boxes.append(boxes[max_idx])\n",
    "\n",
    "        # Calculate intersection over union (IoU) with other boxes\n",
    "        xx1 = np.maximum(x1[max_idx], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[max_idx], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[max_idx], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[max_idx], y2[order[1:]])\n",
    "\n",
    "        intersection = np.maximum(0.0, xx2 - xx1 + 1) * np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        iou = intersection / (areas[max_idx] + areas[order[1:]] - intersection)\n",
    "\n",
    "        # Remove boxes with IoU higher than the threshold\n",
    "        if method == 'nms':\n",
    "            selected_indices = np.where(iou <= overlap)[0]\n",
    "        elif method == 'soft-nms':\n",
    "            weights = np.exp(-(iou * iou) / overlap)\n",
    "            scores[order[1:]] *= weights\n",
    "            selected_indices = np.where(scores[order[1:]] > confidence_thresh)[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Supported methods are 'nms' and 'soft-nms'.\")\n",
    "\n",
    "        order = order[selected_indices + 1]\n",
    "\n",
    "    return selected_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc126d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, bboxes, CLASSES=classes, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n",
    "    NUM_CLASS = len(CLASSES)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    hsv_tuples = [(1.0 * x / NUM_CLASS, 1., 1.) for x in range(NUM_CLASS)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(colors)\n",
    "    random.seed(None)\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        score = float(bbox[4])\n",
    "        class_name = bbox[5]\n",
    "        bbox_color = rectangle_colors if rectangle_colors != '' else colors[CLASSES.index(class_name)]\n",
    "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
    "        if bbox_thick < 1: bbox_thick = 1\n",
    "        fontScale = 0.75 * bbox_thick\n",
    "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "        # put object rectangle\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
    "\n",
    "        if show_label:\n",
    "            # get text label\n",
    "            score_str = \" {:.2f}\".format(score) if show_confidence else \"\"\n",
    "\n",
    "            if tracking: score_str = \" \"+str(score)\n",
    "\n",
    "            label = \"{}\".format(class_name) + score_str\n",
    "\n",
    "            # get text size\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                                                                  fontScale, thickness=bbox_thick)\n",
    "            # put filled text rectangle\n",
    "            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
    "\n",
    "            # put text above rectangle\n",
    "            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165ff83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, inception_v3_input) at 0x2DC93E8B0>})\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model\n",
    "print(loaded_model.signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454c15c",
   "metadata": {},
   "source": [
    "##### Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "210eb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Object_tracking(object_detection_model, video_path, output_path, input_size=(416, 416), show=False, CLASSES=classes, score_threshold=0.3, iou_threshold=0.45, rectangle_colors='', Track_only=['cars', 'trucks']):\n",
    "    # Definition of the parameters\n",
    "    max_cosine_distance = 0.5\n",
    "    nn_budget = None\n",
    "    \n",
    "    # Initializing tracker with Subaru CNN\n",
    "    model_filename = \"models/graphs/InceptionCustom/output_graph.pb\"\n",
    "    encoder = gdet.create_box_encoder(model_filename, output_name='Identity:0', batch_size=1)\n",
    "    print(type(encoder))  # Should print \"<class 'function'>\"\n",
    "    metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "    tracker = Tracker(metric)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    vid = cv2.VideoCapture(video_path)  # Read video frames\n",
    "\n",
    "    # By default, VideoCapture returns float instead of int\n",
    "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "    output_filename = \"output.mov\"\n",
    "    output_path = os.path.join(output_path, output_filename)\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (width, height))\n",
    "\n",
    "    key_list = classes\n",
    "    val_list = classes\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = vid.read()  # Read the next frame from the video\n",
    "        print(frame.shape)\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        try:\n",
    "            original_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            original_image = original_image.astype(np.uint8)\n",
    " \n",
    "        except:\n",
    "            break\n",
    "\n",
    "        image_data = preprocess_image(original_image)\n",
    "        image_data = np.expand_dims(image_data, 0)\n",
    "        \n",
    "        image_data = np.transpose(image_data, (0, 3, 1, 2))\n",
    "        image_data = image_data.astype(np.float32)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        object_detection_model.setInput(image_data)\n",
    "        layer_outputs = object_detection_model.forward(object_detection_model.getUnconnectedOutLayersNames())\n",
    "\n",
    "        # Process the output predictions\n",
    "        pred_bbox = postprocess_boxes(layer_outputs, original_image, input_size, score_threshold, CLASSES)\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_bbox = nms(pred_bbox, iou_threshold, method='nms')\n",
    "\n",
    "#         # Extract bboxes to boxes (x, y, width, height), scores and names\n",
    "#         boxes, scores, names = [], [], []\n",
    "#         for bbox in pred_bbox:\n",
    "#             print(\"bbox print:\",bbox)\n",
    "#             class_index = int(float(bbox[5]))  # Convert the class index to an integer\n",
    "#             if len(Track_only) != 0 and class_index < len(CLASSES) and CLASSES[class_index] in Track_only or len(Track_only) == 0:\n",
    "#                 boxes.append([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "#                 scores.append(bbox[4])\n",
    "\n",
    "        boxes, scores, names = [], [], []\n",
    "        for bbox in pred_bbox:\n",
    "            class_name = bbox[4]\n",
    "            print(\"bbox print:\",bbox)\n",
    "            class_index = CLASSES.index(class_name)\n",
    "\n",
    "            print(f\"len(Track_only): {len(Track_only)}\")\n",
    "            print(f\"class_index: {class_index}\")\n",
    "            print(f\"len(CLASSES): {len(CLASSES)}\")\n",
    "\n",
    "            if class_index < len(CLASSES):\n",
    "                print(f\"CLASSES[class_index]: {CLASSES[class_index]}\")\n",
    "                print(f\"CLASSES[class_index] in Track_only: {CLASSES[class_index] in Track_only}\")\n",
    "\n",
    "            if len(Track_only) != 0 and class_index < len(CLASSES) and CLASSES[class_index] in Track_only or len(Track_only) == 0:\n",
    "                boxes.append([float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])])\n",
    "                scores.append(bbox[5])\n",
    "                names.append(bbox[4])\n",
    "\n",
    "        # Obtain all the detections for the given frame\n",
    "        boxes = np.array(boxes)\n",
    "        print(\"boxes print\",boxes)\n",
    "        print(\"boxes shape\",boxes.shape)\n",
    "        print(\"boxes type\",boxes.dtype)\n",
    "\n",
    "        names = np.array(names)\n",
    "        scores = np.array(scores)\n",
    "        features = np.array(encoder(original_image, boxes))\n",
    "        detections = [Detection(bbox, score, feature, class_name) for bbox, score, feature, class_name in\n",
    "                  zip(boxes, scores, features, names)]\n",
    "\n",
    "        # Pass detections to the deepsort object and obtain the track information\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # Obtain info from the tracks\n",
    "        tracked_bboxes = []\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()  # Get the corrected/predicted bounding box\n",
    "            class_name = track.get_class()  # Get the class name of the particular object\n",
    "            tracking_id = track.track_id  # Get the ID for the particular track\n",
    "            index = key_list[val_list.index(class_name)]  # Get the predicted object index by object name\n",
    "            tracked_bboxes.append(bbox.tolist() + [tracking_id, index])  # Structure data that we could use with our draw_bbox function\n",
    "\n",
    "        if len(times) > 0:\n",
    "            ms = sum(times) / len(times) * 1000\n",
    "            fps = 1000 / ms\n",
    "        else:\n",
    "            ms = 0\n",
    "            fps = 0\n",
    "\n",
    "        # Draw detection on frame\n",
    "        image = draw_bbox(original_image, tracked_bboxes, CLASSES=CLASSES, tracking=True)\n",
    "        image = cv2.putText(image, \"Time: {:.1f} FPS\".format(fps), (0, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,\n",
    "                            (0, 0, 255), 2)\n",
    "\n",
    "        if output_path != '':\n",
    "            out.write(image)\n",
    "        if show:\n",
    "            cv2.imshow('output', image)\n",
    "            if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f62946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Object_tracking(object_detection_model,\n",
    "                video_path,\n",
    "                output_path=output_dir,\n",
    "                input_size=(244, 244),\n",
    "                show=True,\n",
    "                CLASSES=classes,\n",
    "                score_threshold=confidence_thresh,\n",
    "                iou_threshold=overlap,\n",
    "                rectangle_colors='',\n",
    "                Track_only = ['Subaru', 'car']\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea779725",
   "metadata": {},
   "source": [
    "### Object Detection & Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=frame_count, unit='frame') as pbar:\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply the ROI mask to the frame\n",
    "        roi_frame = cv2.bitwise_and(frame, mask)\n",
    "\n",
    "        # Perform object detection within the ROI\n",
    "        blob = cv2.dnn.blobFromImage(roi_frame, input_scale, input_size, swapRB=True, crop=False)\n",
    "        object_detection_model.setInput(blob)\n",
    "        layer_outputs = object_detection_model.forward(object_detection_model.getUnconnectedOutLayersNames())\n",
    "\n",
    "        # Process the bounding boxes\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        for output in layer_outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                class_name = classes[class_id]\n",
    "                if class_name == 'car' and confidence > confidence_thresh:\n",
    "                    center_x = int(detection[0] * frame.shape[1])\n",
    "                    center_y = int(detection[1] * frame.shape[0])\n",
    "                    width = int(detection[2] * frame.shape[1])\n",
    "                    height = int(detection[3] * frame.shape[0])\n",
    "                    x = int(center_x - width / 2)\n",
    "                    y = int(center_y - height / 2)\n",
    "                    boxes.append([x, y, width, height])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maximum suppression\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_thresh, overlap)\n",
    "\n",
    "        # Extract the detected object features for tracking\n",
    "        features = np.array(boxes)  # Use bounding boxes as features (replace this with your feature extraction logic)\n",
    "\n",
    "        # Create Detection instances\n",
    "        detections = []\n",
    "        for bbox, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "            detection = Detection(bbox, confidence, class_id)\n",
    "            detections.append(detection)\n",
    "\n",
    "        # Perform object tracking\n",
    "        outputs = tracker.update(features)\n",
    "\n",
    "        for output, bbox in zip(outputs, boxes):\n",
    "            track_id = output[4]\n",
    "\n",
    "            # Draw bounding box and track ID on the frame\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, str(track_id), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7078c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312e73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe8c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

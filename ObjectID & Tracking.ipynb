{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "93643d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorflow.keras.applications import VGG16,InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import normalize,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from deep_sort.deep_sort import tracker as Tracker\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.tools import generate_detections as gdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c47a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eba098",
   "metadata": {},
   "source": [
    "### Loading Trained InceptionV3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0adcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 08:41:48.800878: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-17 08:41:48.801020: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/Inception_v3_custom/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24d6c7",
   "metadata": {},
   "source": [
    "### Loading Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0b0134ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_model = cv2.dnn.readNet(\"models/yolov4-tiny.weights\", \"models/yolov4-tiny.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea4389",
   "metadata": {},
   "source": [
    "### Reading in Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69eb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'Datasets/Video/QueenAnnRoy/moving/test.mov'\n",
    "output_dir = 'Datasets/Video/QueenAnnRoy/stills_test/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9153e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f4ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "input_size = (416, 416)\n",
    "input_scale = 1/255.0\n",
    "confidence_thresh = .1\n",
    "overlap = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809a9a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'bicycle', 'car', 'motorbike', 'aeroplane']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4d6ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<VideoCapture 0x2deecf470>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8793c",
   "metadata": {},
   "source": [
    "### Defining Region of Interest (ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50359ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_coordinates = [(352, 820), (855, 820), (1282, 270),(1082, 270)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d394c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary mask image with the same size as the original image\n",
    "image = cv2.imread('Datasets/Video/QueenAnnRoy/stills_test/frame_000000.jpg')  # Load your image\n",
    "mask = np.zeros_like(image)\n",
    "\n",
    "# Convert the coords to numpy\n",
    "roi_pts = np.array(roi_coordinates, dtype=np.int32)\n",
    "cv2.fillPoly(mask, [roi_pts], (255, 255, 255))  # Set ROI pixels to white (255)\n",
    "\n",
    "roi = cv2.bitwise_and(image, mask)\n",
    "\n",
    "# cv2.imshow('ROI', roi)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9fa0b",
   "metadata": {},
   "source": [
    "### Initializing DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "210eb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Object_tracking(object_detection_model, video_path, output_path=output_dir, input_size=416, show=False, CLASSES=classes, score_threshold=confidence_thresh, iou_threshold=overlap, rectangle_colors='', Track_only=['cars', 'trucks']):\n",
    "    # Definition of the parameters\n",
    "    max_cosine_distance = 0.5\n",
    "    nn_budget = None\n",
    "\n",
    "    # Initializing tracker with Subaru CNN\n",
    "    model_filename = \"/Users/ryan/GA/Portfolio/Subaru-Seattle/models/saved_model.pb\"\n",
    "    encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "    metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "    tracker = Tracker(metric)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    if video_path:\n",
    "        vid = cv2.VideoCapture(video_path)  # Detect on video\n",
    "    else:\n",
    "        vid = cv2.VideoCapture(0)  # Detect from webcam\n",
    "\n",
    "    # By default, VideoCapture returns float instead of int\n",
    "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (width, height))  # Output_path must be .mp4\n",
    "\n",
    "    NUM_CLASS = read_class_names(CLASSES)\n",
    "    key_list = list(NUM_CLASS.keys())\n",
    "    val_list = list(NUM_CLASS.values())\n",
    "    while True:\n",
    "        _, img = vid.read()\n",
    "\n",
    "        try:\n",
    "            original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            break\n",
    "        image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n",
    "        image_data = tf.expand_dims(image_data, 0)\n",
    "\n",
    "        t1 = time.time()\n",
    "        pred_bbox = object_detection_model.predict(image_data)\n",
    "        t2 = time.time()\n",
    "\n",
    "        times.append(t2 - t1)\n",
    "        times = times[-20:]\n",
    "\n",
    "        pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
    "        pred_bbox = tf.concat(pred_bbox, axis=0)\n",
    "\n",
    "        bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n",
    "        bboxes = nms(bboxes, iou_threshold, method='nms')\n",
    "\n",
    "        # Extract bboxes to boxes (x, y, width, height), scores and names\n",
    "        boxes, scores, names = [], [], []\n",
    "        for bbox in bboxes:\n",
    "            if len(Track_only) != 0 and NUM_CLASS[int(bbox[5])] in Track_only or len(Track_only) == 0:\n",
    "                boxes.append([bbox[0].astype(int), bbox[1].astype(int), bbox[2].astype(int) - bbox[0].astype(int),\n",
    "                              bbox[3].astype(int) - bbox[1].astype(int)])\n",
    "                scores.append(bbox[4])\n",
    "                names.append(NUM_CLASS[int(bbox[5])])\n",
    "\n",
    "        # Obtain all the detections for the given frame\n",
    "        boxes = np.array(boxes)\n",
    "        names = np.array(names)\n",
    "        scores = np.array(scores)\n",
    "        features = np.array(encoder(original_image, boxes))\n",
    "        detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                      zip(boxes, scores, names, features)]\n",
    "\n",
    "        # Pass detections to the deepsort object and obtain the track information\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        # Obtain info from the tracks\n",
    "        tracked_bboxes = []\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()  # Get the corrected/predicted bounding box\n",
    "            class_name = track.get_class()  # Get the class name of the particular object\n",
    "            tracking_id = track.track_id  # Get the ID for the particular track\n",
    "            index = key_list[val_list.index(class_name)]  # Get the predicted object index by object name\n",
    "            tracked_bboxes.append(bbox.tolist() + [tracking_id, index])  # Structure data that we could use with our draw_bbox function\n",
    "\n",
    "        ms = sum(times) / len(times) * 1000\n",
    "        fps = 1000 / ms\n",
    "\n",
    "        # Draw detection on frame\n",
    "        image = draw_bbox(original_image, tracked_bboxes, CLASSES=CLASSES, tracking=True)\n",
    "        image = cv2.putText(image, \"Time: {:.1f} FPS\".format(fps), (0, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,\n",
    "                            (0, 0, 255), 2)\n",
    "\n",
    "        if output_path != '':\n",
    "            out.write(image)\n",
    "        if show:\n",
    "            cv2.imshow('output', image)\n",
    "            if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "22f62946",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: /Users/ryan/GA/Portfolio/Subaru-Seattle/models/saved_model.pb/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mObject_tracking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_detection_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrectangle_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mTrack_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcars\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrucks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[195], line 8\u001b[0m, in \u001b[0;36mObject_tracking\u001b[0;34m(object_detection_model, video_path, output_path, input_size, show, CLASSES, score_threshold, iou_threshold, rectangle_colors, Track_only)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initializing tracker with Subaru CNN\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ryan/GA/Portfolio/Subaru-Seattle/models/saved_model.pb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mgdet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_box_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m metric \u001b[38;5;241m=\u001b[39m nn_matching\u001b[38;5;241m.\u001b[39mNearestNeighborDistanceMetric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_cosine_distance, nn_budget)\n\u001b[1;32m     10\u001b[0m tracker \u001b[38;5;241m=\u001b[39m Tracker(metric)\n",
      "File \u001b[0;32m~/GA/Portfolio/Subaru-Seattle/deep_sort/tools/generate_detections.py:90\u001b[0m, in \u001b[0;36mcreate_box_encoder\u001b[0;34m(model_filename, input_name, output_name, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_box_encoder\u001b[39m(model_filename, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m---> 90\u001b[0m     image_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mImageEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     image_shape \u001b[38;5;241m=\u001b[39m image_encoder\u001b[38;5;241m.\u001b[39mimage_shape\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(image, boxes):\n",
      "File \u001b[0;32m~/GA/Portfolio/Subaru-Seattle/deep_sort/tools/generate_detections.py:73\u001b[0m, in \u001b[0;36mImageEncoder.__init__\u001b[0;34m(self, checkpoint_filename, input_name, output_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_filename, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msignatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_tensor_name \u001b[38;5;241m=\u001b[39m input_name\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:782\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[1;32m    781\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[0;32m--> 782\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:887\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, \u001b[38;5;28mset\u001b[39m):\n\u001b[1;32m    883\u001b[0m   \u001b[38;5;66;03m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[1;32m    884\u001b[0m   \u001b[38;5;66;03m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m   tags \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(tags)\n\u001b[1;32m    886\u001b[0m saved_model_proto, debug_info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 887\u001b[0m     \u001b[43mloader_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_saved_model_with_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_graph_def\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    891\u001b[0m   metrics\u001b[38;5;241m.\u001b[39mIncrementReadApi(_LOAD_V2_LABEL)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py:57\u001b[0m, in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_saved_model_with_debug_info\u001b[39m(export_dir):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m   saved_model \u001b[38;5;241m=\u001b[39m \u001b[43mparse_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m   debug_info_path \u001b[38;5;241m=\u001b[39m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     60\u001b[0m       saved_model_utils\u001b[38;5;241m.\u001b[39mget_debug_dir(export_dir),\n\u001b[1;32m     61\u001b[0m       constants\u001b[38;5;241m.\u001b[39mDEBUG_INFO_FILENAME_PB)\n\u001b[1;32m     62\u001b[0m   debug_info \u001b[38;5;241m=\u001b[39m graph_debug_info_pb2\u001b[38;5;241m.\u001b[39mGraphDebugInfo()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py:115\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot parse file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_pbtxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel file does not exist at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /Users/ryan/GA/Portfolio/Subaru-Seattle/models/saved_model.pb/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "Object_tracking(object_detection_model,\n",
    "                    video_path,\n",
    "                    output_path=output_dir,\n",
    "                    input_size=416,\n",
    "                    show=False,\n",
    "                    CLASSES=classes,\n",
    "                    score_threshold=confidence_thresh,\n",
    "                    iou_threshold=overlap,\n",
    "                    rectangle_colors='',\n",
    "                    Track_only=['cars', 'trucks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea779725",
   "metadata": {},
   "source": [
    "### Object Detection & Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=frame_count, unit='frame') as pbar:\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply the ROI mask to the frame\n",
    "        roi_frame = cv2.bitwise_and(frame, mask)\n",
    "\n",
    "        # Perform object detection within the ROI\n",
    "        blob = cv2.dnn.blobFromImage(roi_frame, input_scale, input_size, swapRB=True, crop=False)\n",
    "        object_detection_model.setInput(blob)\n",
    "        layer_outputs = object_detection_model.forward(object_detection_model.getUnconnectedOutLayersNames())\n",
    "\n",
    "        # Process the bounding boxes\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        for output in layer_outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                class_name = classes[class_id]\n",
    "                if class_name == 'car' and confidence > confidence_thresh:\n",
    "                    center_x = int(detection[0] * frame.shape[1])\n",
    "                    center_y = int(detection[1] * frame.shape[0])\n",
    "                    width = int(detection[2] * frame.shape[1])\n",
    "                    height = int(detection[3] * frame.shape[0])\n",
    "                    x = int(center_x - width / 2)\n",
    "                    y = int(center_y - height / 2)\n",
    "                    boxes.append([x, y, width, height])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maximum suppression\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_thresh, overlap)\n",
    "\n",
    "        # Extract the detected object features for tracking\n",
    "        features = np.array(boxes)  # Use bounding boxes as features (replace this with your feature extraction logic)\n",
    "\n",
    "        # Create Detection instances\n",
    "        detections = []\n",
    "        for bbox, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "            detection = Detection(bbox, confidence, class_id)\n",
    "            detections.append(detection)\n",
    "\n",
    "        # Perform object tracking\n",
    "        outputs = tracker.update(features)\n",
    "\n",
    "        for output, bbox in zip(outputs, boxes):\n",
    "            track_id = output[4]\n",
    "\n",
    "            # Draw bounding box and track ID on the frame\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, str(track_id), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7078c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312e73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe8c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79e271d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c76224",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46223cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import normalize,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1dc531",
   "metadata": {},
   "source": [
    "### Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b27197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398386d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59301e2",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "To Train-Test split the data, we'll be seperating X on index to prevent all of the data from being loaded into the kernel concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb63954",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx, y_train, y_val = train_test_split(idx, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959dcb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fae4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210bf5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228524"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c1d532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f485ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "\n",
    "y_train_int = np.array([label_map[label] for label in y_train])\n",
    "y_val_int = np.array([label_map[label] for label in y_val])\n",
    "\n",
    "y_train = to_categorical(y_train_int, num_classes=75)\n",
    "y_val = to_categorical(y_val_int, num_classes=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b396c",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "\n",
    "We'll feed our data into the model in batches of 32 so that all images arenâ€™t loaded into the kernel at the same time. Within this function, we'll divide X_low by 255, to normalize the data.\n",
    "\n",
    "This operation is split into two seperate generators to prevent data leakage from the training set into the validation set. Since the function has been seperated, each function will only be called when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f97fc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(X_train, y_train, batch_size):\n",
    "    num_train_samples = len(X_train)\n",
    "    train_indices = np.arange(num_train_samples)\n",
    "    np.random.shuffle(train_indices)\n",
    "\n",
    "    while True:\n",
    "        for start_idx in range(0, num_train_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_train_samples)\n",
    "            batch_indices = train_indices[start_idx:end_idx]\n",
    "            X_batch_train = X_train[batch_indices]\n",
    "            y_batch_train = y_train[batch_indices]\n",
    "\n",
    "            # Normalize the input data to [0, 1]\n",
    "            X_batch_train = X_batch_train.astype('float32') / 255.0\n",
    "\n",
    "            yield (X_batch_train, y_batch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58a6e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_data_generator(X_val, y_val, batch_size):\n",
    "    num_val_samples = len(X_val)\n",
    "    val_indices = np.arange(num_val_samples)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    while True:\n",
    "        for start_idx_val in range(0, num_val_samples, batch_size):\n",
    "            end_idx_val = min(start_idx_val + batch_size, num_val_samples)\n",
    "            batch_indices_val = val_indices[start_idx_val:end_idx_val]\n",
    "            X_batch_val = X_val[batch_indices_val]\n",
    "            y_batch_val = y_val[batch_indices_val]\n",
    "\n",
    "            # Normalize the input\n",
    "            X_batch_val = X_batch_val.astype('float32') / 255.0\n",
    "\n",
    "            yield (X_batch_val, y_batch_val)\n",
    "\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_data_generator(X_train, y_train, batch_size)\n",
    "val_generator = val_data_generator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57426bd2",
   "metadata": {},
   "source": [
    "### Instantiating the Model\n",
    "\n",
    "Now we can instantiate the model. This time we'll use a slightly different architecture than our previous neural net, as we'll need additional regularization to account for the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f66013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.1), input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.1)),\n",
    "    MaxPooling2D(2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(.2),\n",
    "    Dense(75, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51db5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c0b189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 15:15:48.275388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7141/7141 [==============================] - ETA: 0s - loss: 3.1983 - accuracy: 0.1400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 15:28:00.308347: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7141/7141 [==============================] - 877s 123ms/step - loss: 3.1983 - accuracy: 0.1400 - val_loss: 3.1415 - val_accuracy: 0.1467\n",
      "Epoch 2/100\n",
      "7141/7141 [==============================] - 844s 118ms/step - loss: 3.1885 - accuracy: 0.1449 - val_loss: 3.2114 - val_accuracy: 0.1529\n",
      "Epoch 3/100\n",
      "7141/7141 [==============================] - 878s 123ms/step - loss: 3.2741 - accuracy: 0.1529 - val_loss: 3.2834 - val_accuracy: 0.1578\n",
      "Epoch 4/100\n",
      "7141/7141 [==============================] - 881s 123ms/step - loss: 3.3142 - accuracy: 0.1572 - val_loss: 3.2877 - val_accuracy: 0.1638\n",
      "Epoch 5/100\n",
      "7141/7141 [==============================] - 875s 123ms/step - loss: 3.3462 - accuracy: 0.1604 - val_loss: 3.3293 - val_accuracy: 0.1655\n",
      "Epoch 6/100\n",
      "7141/7141 [==============================] - 852s 119ms/step - loss: 3.3749 - accuracy: 0.1621 - val_loss: 3.3703 - val_accuracy: 0.1664\n",
      "Epoch 7/100\n",
      "7141/7141 [==============================] - 882s 123ms/step - loss: 3.4012 - accuracy: 0.1632 - val_loss: 3.4314 - val_accuracy: 0.1656\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // batch_size,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97312e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
